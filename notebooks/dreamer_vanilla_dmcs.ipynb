{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# %env MUJOCO_GL=osmesa\n",
    "# !export MUJOCO_EGL_DEVICE_ID="
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms import dreamer\n",
    "from ray.rllib.env.wrappers import dm_control_wrapper\n",
    "from ray.tune import registry"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.int = int\n",
    "def env_creator(_):\n",
    "    env = dm_control_wrapper.DMCEnv(\n",
    "        'cartpole',\n",
    "        'swingup',\n",
    "        from_pixels=True,\n",
    "        height=64,\n",
    "        width=64,\n",
    "        frame_skip=2,\n",
    "        channels_first=True\n",
    "    )\n",
    "    return env\n",
    "registry.register_env('dmc', env_creator)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 21:39:28,518\tINFO trainable.py:172 -- Trainable.setup took 15.884 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2023-02-16 21:39:28,521\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "algo = (\n",
    "    dreamer.DreamerConfig()\n",
    "    .framework('torch')\n",
    "    .resources(num_gpus=0)\n",
    "    .environment('dmc')\n",
    "    .build())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-iteration=0/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "CUDA out of memory. Tried to allocate 286.10 GiB (GPU 0; 7.77 GiB total capacity; 2.90 GiB already allocated; 1.76 GiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n tracebackTraceback (most recent call last):\n  File \"/home/miles/anaconda3/envs/distracting_benchmarks/lib/python3.8/site-packages/ray/rllib/policy/torch_policy_v2.py\", line 1128, in _worker\n    self.loss(model, self.dist_class, sample_batch)\n  File \"/home/miles/anaconda3/envs/distracting_benchmarks/lib/python3.8/site-packages/ray/rllib/algorithms/dreamer/dreamer_torch_policy.py\", line 76, in loss\n    image_loss = -torch.mean(image_pred.log_prob(train_batch[\"obs\"]))\n  File \"/home/miles/anaconda3/envs/distracting_benchmarks/lib/python3.8/site-packages/torch/distributions/independent.py\", line 99, in log_prob\n    log_prob = self.base_dist.log_prob(value)\n  File \"/home/miles/anaconda3/envs/distracting_benchmarks/lib/python3.8/site-packages/torch/distributions/normal.py\", line 83, in log_prob\n    return -((value - self.loc) ** 2) / (2 * var) - log_scale - math.log(math.sqrt(2 * math.pi))\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 286.10 GiB (GPU 0; 7.77 GiB total capacity; 2.90 GiB already allocated; 1.76 GiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\nIn tower 0 on device cuda:0",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "\u001B[0;32m~/anaconda3/envs/distracting_benchmarks/lib/python3.8/site-packages/ray/rllib/policy/torch_policy_v2.py\u001B[0m in \u001B[0;36m_worker\u001B[0;34m(shard_idx, model, sample_batch, device)\u001B[0m\n\u001B[1;32m   1127\u001B[0m                     loss_out = force_list(\n\u001B[0;32m-> 1128\u001B[0;31m                         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdist_class\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msample_batch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1129\u001B[0m                     )\n",
      "\u001B[0;32m~/anaconda3/envs/distracting_benchmarks/lib/python3.8/site-packages/ray/rllib/algorithms/dreamer/dreamer_torch_policy.py\u001B[0m in \u001B[0;36mloss\u001B[0;34m(self, model, dist_class, train_batch)\u001B[0m\n\u001B[1;32m     75\u001B[0m         \u001B[0mreward_pred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 76\u001B[0;31m         \u001B[0mimage_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimage_pred\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlog_prob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_batch\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"obs\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     77\u001B[0m         \u001B[0mreward_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreward_pred\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlog_prob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_batch\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"rewards\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/distracting_benchmarks/lib/python3.8/site-packages/torch/distributions/independent.py\u001B[0m in \u001B[0;36mlog_prob\u001B[0;34m(self, value)\u001B[0m\n\u001B[1;32m     98\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mlog_prob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 99\u001B[0;31m         \u001B[0mlog_prob\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbase_dist\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlog_prob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    100\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0m_sum_rightmost\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlog_prob\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreinterpreted_batch_ndims\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/distracting_benchmarks/lib/python3.8/site-packages/torch/distributions/normal.py\u001B[0m in \u001B[0;36mlog_prob\u001B[0;34m(self, value)\u001B[0m\n\u001B[1;32m     82\u001B[0m         \u001B[0mlog_scale\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlog\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mscale\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mscale\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mReal\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mscale\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlog\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 83\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m**\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mvar\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mlog_scale\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mmath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlog\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqrt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mmath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpi\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     84\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 286.10 GiB (GPU 0; 7.77 GiB total capacity; 2.90 GiB already allocated; 1.76 GiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-f38e2f7e52ae>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0malgo\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/anaconda3/envs/distracting_benchmarks/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    365\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    366\u001B[0m             \u001B[0mskipped\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mskip_exceptions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 367\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mskipped\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mexception_cause\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mskipped\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    368\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    369\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"step() needs to return a dict.\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/distracting_benchmarks/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    362\u001B[0m         \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    363\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 364\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    365\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    366\u001B[0m             \u001B[0mskipped\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mskip_exceptions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/distracting_benchmarks/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    747\u001B[0m         \u001B[0;31m#   evaluate after the training iteration is entirely done.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    748\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 749\u001B[0;31m             \u001B[0mresults\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_iter_ctx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_one_training_iteration\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    750\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    751\u001B[0m         \u001B[0;31m# Sequential: Train (already done above), then evaluate.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/distracting_benchmarks/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py\u001B[0m in \u001B[0;36m_run_one_training_iteration\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2621\u001B[0m                 \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_timers\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mTRAINING_ITERATION_TIMER\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2622\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_disable_execution_plan_api\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2623\u001B[0;31m                         \u001B[0mresults\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtraining_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2624\u001B[0m                     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2625\u001B[0m                         \u001B[0mresults\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_exec_impl\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/distracting_benchmarks/lib/python3.8/site-packages/ray/rllib/algorithms/dreamer/dreamer.py\u001B[0m in \u001B[0;36mtraining_step\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    386\u001B[0m                 \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"sub-iteration={n}/{dreamer_train_iters}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    387\u001B[0m                 \u001B[0mbatch\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlocal_replay_buffer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msample\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 388\u001B[0;31m                 \u001B[0mfetches\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlocal_worker\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlearn_on_batch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    389\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    390\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mfetches\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/distracting_benchmarks/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\u001B[0m in \u001B[0;36mlearn_on_batch\u001B[0;34m(self, samples)\u001B[0m\n\u001B[1;32m   1019\u001B[0m                 info_out.update(\n\u001B[1;32m   1020\u001B[0m                     {\n\u001B[0;32m-> 1021\u001B[0;31m                         DEFAULT_POLICY_ID: self.policy_map[\n\u001B[0m\u001B[1;32m   1022\u001B[0m                             \u001B[0mDEFAULT_POLICY_ID\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1023\u001B[0m                         ].learn_on_batch(samples)\n",
      "\u001B[0;32m~/anaconda3/envs/distracting_benchmarks/lib/python3.8/site-packages/ray/rllib/utils/threading.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(self, *a, **k)\u001B[0m\n\u001B[1;32m     22\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 24\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     25\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mAttributeError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0;34m\"has no attribute '_lock'\"\u001B[0m \u001B[0;32min\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/distracting_benchmarks/lib/python3.8/site-packages/ray/rllib/policy/torch_policy_v2.py\u001B[0m in \u001B[0;36mlearn_on_batch\u001B[0;34m(self, postprocessed_batch)\u001B[0m\n\u001B[1;32m    614\u001B[0m         \u001B[0;31m# Compute gradients (will calculate all losses and `backward()`\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    615\u001B[0m         \u001B[0;31m# them to get the grads).\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 616\u001B[0;31m         \u001B[0mgrads\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfetches\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompute_gradients\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpostprocessed_batch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    617\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    618\u001B[0m         \u001B[0;31m# Step the optimizers.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/distracting_benchmarks/lib/python3.8/site-packages/ray/rllib/utils/threading.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(self, *a, **k)\u001B[0m\n\u001B[1;32m     22\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 24\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     25\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mAttributeError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0;34m\"has no attribute '_lock'\"\u001B[0m \u001B[0;32min\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/distracting_benchmarks/lib/python3.8/site-packages/ray/rllib/policy/torch_policy_v2.py\u001B[0m in \u001B[0;36mcompute_gradients\u001B[0;34m(self, postprocessed_batch)\u001B[0m\n\u001B[1;32m    814\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    815\u001B[0m         \u001B[0;31m# Do the (maybe parallelized) gradient calculation step.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 816\u001B[0;31m         \u001B[0mtower_outputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_multi_gpu_parallel_grad_calc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpostprocessed_batch\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    817\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    818\u001B[0m         \u001B[0mall_grads\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtower_outputs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/distracting_benchmarks/lib/python3.8/site-packages/ray/rllib/policy/torch_policy_v2.py\u001B[0m in \u001B[0;36m_multi_gpu_parallel_grad_calc\u001B[0;34m(self, sample_batches)\u001B[0m\n\u001B[1;32m   1210\u001B[0m                 \u001B[0mlast_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mresults\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresults\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1211\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlast_result\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1212\u001B[0;31m                     \u001B[0;32mraise\u001B[0m \u001B[0mlast_result\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mlast_result\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1213\u001B[0m         \u001B[0;31m# Multi device (GPU) case: Parallelize via threads.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1214\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: CUDA out of memory. Tried to allocate 286.10 GiB (GPU 0; 7.77 GiB total capacity; 2.90 GiB already allocated; 1.76 GiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n tracebackTraceback (most recent call last):\n  File \"/home/miles/anaconda3/envs/distracting_benchmarks/lib/python3.8/site-packages/ray/rllib/policy/torch_policy_v2.py\", line 1128, in _worker\n    self.loss(model, self.dist_class, sample_batch)\n  File \"/home/miles/anaconda3/envs/distracting_benchmarks/lib/python3.8/site-packages/ray/rllib/algorithms/dreamer/dreamer_torch_policy.py\", line 76, in loss\n    image_loss = -torch.mean(image_pred.log_prob(train_batch[\"obs\"]))\n  File \"/home/miles/anaconda3/envs/distracting_benchmarks/lib/python3.8/site-packages/torch/distributions/independent.py\", line 99, in log_prob\n    log_prob = self.base_dist.log_prob(value)\n  File \"/home/miles/anaconda3/envs/distracting_benchmarks/lib/python3.8/site-packages/torch/distributions/normal.py\", line 83, in log_prob\n    return -((value - self.loc) ** 2) / (2 * var) - log_scale - math.log(math.sqrt(2 * math.pi))\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 286.10 GiB (GPU 0; 7.77 GiB total capacity; 2.90 GiB already allocated; 1.76 GiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\nIn tower 0 on device cuda:0"
     ]
    }
   ],
   "source": [
    "algo.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
